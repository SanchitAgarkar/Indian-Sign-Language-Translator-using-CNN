{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e38f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1562b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fadd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio():\n",
    "    model_dir = 'Saved Models\\\\AlexNet_Saved_Thu_Feb_10_22_02_19.json'\n",
    "    model_weights_dir = 'Saved Models\\\\AlexNet_Saved_weights_Thu_Feb_10_22_02_19.hdf5'\n",
    "    with open(model_dir, 'r') as json_file:\n",
    "        json_saved_model = json_file.read()\n",
    "    model = tf.keras.models.model_from_json(json_saved_model)\n",
    "    model.load_weights(model_weights_dir) \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    t,r,b,l=100,350,325,575 #size of window\n",
    "    visual_dict={0:'1',1:'2',2:'3',3:'A',4:'B',5:'C',6:'J',7:'My',8:'Name',9:'Y'}\n",
    "\n",
    "    import pyttsx3\n",
    "    def text_audio(text):\n",
    "        obj = pyttsx3.init()\n",
    "        voices = obj.getProperty('voices')\n",
    "        obj.setProperty(\"voice\",voices[1].id)\n",
    "        obj. setProperty(\"rate\", 150)\n",
    "        speech = text\n",
    "        obj.say(speech)\n",
    "        obj.runAndWait()\n",
    "    camera = cv2.VideoCapture(0) #Created object \n",
    "    num_frames = 0\n",
    "\n",
    "    while(True):\n",
    "        # Capture the video frame by frame\n",
    "        success, frame = camera.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        roi = frame[140:380, 235:440]\n",
    "        cv2.rectangle(frame, (225,135), (450,400), (0,255,0), 2)\n",
    "        clone=frame.copy()\n",
    "\n",
    "\n",
    "        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "        lh = 0\n",
    "        ls = 0\n",
    "        lv = 0\n",
    "        uh = 255\n",
    "        us = 32\n",
    "        uv = 255\n",
    "\n",
    "        cv2.putText(clone, \"Put your hand gestures in the rectangle\", (5, 25), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "        cv2.putText(clone, \"Press 'q' to exit.\", (5, 50), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))\n",
    "        #Storing the lower and upper bound in an array\n",
    "        l_b = np.array([lh, ls, lv])\n",
    "        u_b = np.array([uh, us, uv])\n",
    "\n",
    "        mask = cv2.inRange(hsv, l_b, u_b)\n",
    "        mask = cv2.bitwise_not(mask)\n",
    "        res = cv2.bitwise_and(roi, roi, mask=mask)\n",
    "        res = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        (cnts, _) = cv2.findContours(res, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(cnts)>0:\n",
    "            # Storing the max contors (Which area is maximum)\n",
    "            max_cont = max(cnts, key=cv2.contourArea)\n",
    "            if max_cont is not None:\n",
    "                mask = cv2.drawContours(res, [max_cont + (r, t)], -1, (0, 0, 255))\n",
    "                mask = np.zeros(res.shape, dtype=\"uint8\")\n",
    "                cv2.drawContours(mask, [max_cont], -1, 255, -1)\n",
    "                #cv2.imshow('1_normal mask', mask)\n",
    "                res = cv2.bitwise_and(res, res, mask=mask)\n",
    "                #cv2.imshow('2_res', res)\n",
    "                high_thresh, thresh_im = cv2.threshold(res, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                lowThresh = 0.5 * high_thresh\n",
    "                res = cv2.Canny(res, lowThresh, high_thresh)\n",
    "                cv2.imshow('Canny', res)\n",
    "                # Prediction\n",
    "                if res is not None and cv2.contourArea(max_cont) > 1000:\n",
    "                    final_res = cv2.resize(res, (224,224))\n",
    "                    final_res = final_res / 255\n",
    "                    final_res = final_res.reshape(-1, 224, 224, 1)\n",
    "                    output = model.predict(final_res)\n",
    "                    prob = np.amax(output)\n",
    "                    sign = np.argmax(output)#gives the index of maximum vale of in array\n",
    "                    final_sign = visual_dict[sign]\n",
    "                    text_audio(final_sign)\n",
    "                    cv2.putText(clone, 'Sign : ' + str(final_sign), (175, 90), cv2.FONT_HERSHEY_COMPLEX, 2,(0, 0, 255))\n",
    "        num_frames += 1\n",
    "        cv2.imshow(\"Webcam\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\") or keypress == ord(\"Q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2cdfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_audio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
